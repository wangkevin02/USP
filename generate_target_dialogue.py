import json
import jsonlines
import numpy as np
import torch
import os
import hashlib
from transformers import AutoTokenizer, AutoModel
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cosine
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import random
from matplotlib.colors import TABLEAU_COLORS
from scipy.stats import gaussian_kde, mannwhitneyu
import matplotlib.lines as mlines

class DialogueAnalyzer:
    def __init__(
        self, 
        model_name: str = "princeton-nlp/sup-simcse-roberta-large",
        batch_size: int = 32,
        cache_dir: str = "embedding_cache",
        model_dir: str = "./model_cache"
    ):
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = batch_size
        self.cache_dir = cache_dir
        
        print("â³ Loading model...")
        start = time.time()
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModel.from_pretrained(model_dir).to(self.device)
        print(f"âœ… Model loaded | Time: {time.time()-start:.1f}s")
        self.model.eval()
        os.makedirs(cache_dir, exist_ok=True)

    def _get_dialogue_text(self, dialogue):
        """Concatenate all text content in the dialogue"""
        return " ".join([turn["content"] for turn in dialogue if turn["role"] != "system"])

    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling to get sentence embeddings"""
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def _get_cache_path(self, text):
        """Generate cache file path"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        return os.path.join(self.cache_dir, f"{text_hash}.npy")

    def _get_cached_embedding(self, text):
        """Get embedding from cache"""
        cache_path = self._get_cache_path(text)
        if os.path.exists(cache_path):
            return np.load(cache_path)
        return None

    def _save_to_cache(self, text, embedding):
        """Save embedding to cache"""
        cache_path = self._get_cache_path(text)
        np.save(cache_path, embedding)

    def get_embeddings(self, texts):
        """Generate text embeddings in batch"""
        embeddings = []
        for i in tqdm(range(0, len(texts), self.batch_size), desc="Generating embeddings"):
            batch = texts[i:i+self.batch_size]
            inputs = self.tokenizer(
                batch, 
                padding=True, 
                truncation=True, 
                max_length=512, 
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                batch_embeddings = self._mean_pooling(outputs, inputs['attention_mask'])
                embeddings.append(batch_embeddings.cpu().numpy())
        
        return np.concatenate(embeddings)

    def get_embeddings_with_cache(self, texts):
        """Generate embeddings with caching"""
        embeddings = []
        texts_to_process = []
        indices_to_process = []

        for i, text in enumerate(texts):
            cached_embedding = self._get_cached_embedding(text)
            if cached_embedding is not None:
                embeddings.append(cached_embedding)
            else:
                texts_to_process.append(text)
                indices_to_process.append(i)

        if texts_to_process:
            new_embeddings = self.get_embeddings(texts_to_process)
            
            for text, embedding in zip(texts_to_process, new_embeddings):
                self._save_to_cache(text, embedding)
            
            for idx, embedding in zip(indices_to_process, new_embeddings):
                embeddings.insert(idx, embedding)

        return np.array(embeddings)

    def analyze_dialogues(self, file_path):
        """ä¿®æ”¹åçš„åˆ†ææ–¹æ³•ï¼Œä½¿ç”¨äºŒç»´PCA"""
        target_dialogues = []
        generated_dialogues = {}
        
        with jsonlines.open(file_path) as reader:
            all_models = set()
            for obj in reader:
                if "models" in obj:
                    all_models.update(obj["models"].keys())
            reader.close()
            
            for model in all_models:
                generated_dialogues[model] = []
            
            with jsonlines.open(file_path) as reader:
                for obj in tqdm(reader, desc="Reading data"):
                    if "target_dialogue" in obj:
                        target_text = self._get_dialogue_text(obj["target_dialogue"])
                        if target_text.strip():
                            target_dialogues.append(target_text)
                    
                    if "models" in obj:
                        for model_name in all_models:
                            if model_name in obj["models"]:
                                dialogue = obj["models"][model_name]
                                text = self._get_dialogue_text(dialogue)
                                generated_dialogues[model_name].append(text if text.strip() else None)

        print(f"\nFound {len(target_dialogues)} target dialogues")
        target_embeddings = self.get_embeddings_with_cache([d for d in target_dialogues if d])
        
        generated_embeddings = {}
        for model_name, dialogues in generated_dialogues.items():
            valid_dialogues = [d for d in dialogues if d]
            if valid_dialogues:
                embeddings = self.get_embeddings_with_cache(valid_dialogues)
                generated_embeddings[model_name] = embeddings

        # äºŒç»´PCAé™ç»´
        all_embeddings = np.concatenate([target_embeddings] + [e for e in generated_embeddings.values()])
        scaler = StandardScaler()
        scaled_embeddings = scaler.fit_transform(all_embeddings)
        
        pca = PCA(n_components=2)
        pca_embeddings = pca.fit_transform(scaled_embeddings)
        
        # åˆ†å‰²ç»“æœ
        target_pca = pca_embeddings[:len(target_embeddings)]
        pca_results = {}
        start = len(target_embeddings)
        for model_name in generated_embeddings:
            end = start + len(generated_embeddings[model_name])
            pca_results[model_name] = pca_embeddings[start:end]
            start = end

        return {
            'target_embeddings': target_pca,
            'generated_embeddings': pca_results
        }

    
    def visualize_2d_differences(self, analysis_results, output_file="2d_diff_visualization.pdf"):
        """
        ä¼˜åŒ–ç‰ˆçš„äºŒç»´å·®å¼‚å¯è§†åŒ–ï¼Œé€‰æ‹©60%ä½ç½®çš„æœ€å°å€¼ç‚¹è¿›è¡Œæ ‡æ³¨
        """
        if not analysis_results:
            print("No analysis results to visualize!")
            return
    
        # è®¾ç½®å­—ä½“
        plt.rcParams['font.family'] = 'DejaVu Serif'
        
        plt.figure(figsize=(16, 10))
        
        # é…ç½®é¢œè‰²å’Œæ ·å¼
        colors = plt.cm.tab10.colors
        line_styles = ['-', '--', '-.', ':', (0, (3, 1, 1, 1))]
        marker_styles = ['o', 's', '^', 'D', 'P']
        
        # åˆ›å»ºåæ ‡è½´
        ax = plt.gca()
        
        # å­˜å‚¨æ‰€æœ‰æ¨¡å‹åœ¨60%ä½ç½®çš„å€¼
        values_at_60 = {}
        all_curves = {}
        
        # å®šä¹‰æœŸæœ›çš„æ¨¡å‹é¡ºåº
        desired_order = ['ProfileGPT(4o)', 'DialogueGPT(4o)', 'PlatoLM', 'USP w/o RLCC', 'USP']
        
        # æ¨¡å‹åç§°æ˜ å°„
        model_name_map = {
            'gpt4o_profile': 'ProfileGPT(4o)',
            'gpt4o_dialoge': 'DialogueGPT(4o)',
            'plato': 'PlatoLM',
            'ppo': 'USP',
            'us_without_ppo': 'USP w/o RLCC'
        }
        
        # åå‘æ˜ å°„ï¼Œç”¨äºæ‰¾åˆ°å¯¹åº”çš„åŸå§‹é”®å
        reverse_map = {v: k for k, v in model_name_map.items()}
        
        # é¢„å¤„ç†æ•°æ®å¹¶ç»˜åˆ¶ä¸»æ›²çº¿
        max_x = 0
        
        # æŒ‰ç…§æœŸæœ›é¡ºåºç»˜åˆ¶æ›²çº¿
        for model_idx, desired_name in enumerate(desired_order):
            # è·å–åŸå§‹æ¨¡å‹åç§°
            original_name = reverse_map[desired_name]
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
            if original_name not in analysis_results['generated_embeddings']:
                continue
                
            embeddings = analysis_results['generated_embeddings'][original_name]
            
            # è®¡ç®—ç»å¯¹å·®å¼‚
            target = analysis_results['target_embeddings'][:len(embeddings)]
            differences = np.linalg.norm(embeddings - target, axis=1)
            sorted_diff = np.sort(np.abs(differences))
            
            # è®¡ç®—åˆ†ä½ç‚¹ï¼ˆæ¯5%ä¸€ä¸ªç‚¹ï¼‰
            percentiles = np.linspace(0, 100, 21)
            quantiles = np.percentile(sorted_diff, percentiles)
            
            # å­˜å‚¨æ›²çº¿æ•°æ®
            all_curves[desired_name] = (percentiles, quantiles)
            
            # æ‰¾åˆ°æœ€æ¥è¿‘60%çš„å€¼
            idx_60 = np.abs(percentiles - 60).argmin()
            values_at_60[desired_name] = quantiles[idx_60]
            
            # ç»˜åˆ¶ä¸»æ›²çº¿
            ax.plot(percentiles, quantiles, 
                    color=colors[model_idx],
                    linestyle=line_styles[model_idx % len(line_styles)],
                    marker=marker_styles[model_idx % len(marker_styles)],
                    markersize=8,
                    linewidth=2.0,
                    label=desired_name)
            
            max_x = max(max_x, np.max(sorted_diff))
        
        # æ‰¾åˆ°60%ä½ç½®çš„æœ€å°å€¼
        max_model = max(values_at_60.items(), key=lambda x: x[1])
        max_model_name = max_model[0]
        max_value = max_model[1]
        
        # æ·»åŠ å±€éƒ¨å‚è€ƒçº¿ï¼ˆåªåœ¨ç‚¹çš„å·¦ä¾§å’Œä¸‹ä¾§ï¼‰
        # å‚ç›´å‚è€ƒçº¿ï¼ˆä»æœ€å°å€¼ç‚¹åˆ°xè½´ï¼‰
        ax.plot([60, 60], [0, max_value], 
                color='gray', linestyle='--', alpha=0.7, linewidth=1.5)
        
        # æ°´å¹³å‚è€ƒçº¿ï¼ˆä»æœ€å°å€¼ç‚¹åˆ°yè½´ï¼‰
        ax.plot([0, 60], [max_value, max_value], 
                color='gray', linestyle='--', alpha=0.7, linewidth=1.5)
        
        # ä½¿ç”¨æ˜Ÿå½¢æ ‡è®°æ›¿ä»£scatterç‚¹å’Œæ ‡æ³¨
        ax.plot(60, max_value, marker='X', color='red', markersize=25, zorder=5)
        
        # åæ ‡è½´è®¾ç½®
        ax.set_xlabel('Percentile (%)', fontsize=26)
        ax.set_ylabel('Absolute Difference Value', fontsize=26)
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, 100)
        ax.set_ylim(0, max_x*1.1)
        
        # å¢å¤§åˆ»åº¦å­—ä½“
        ax.tick_params(axis='both', labelsize=22)
        
        # åˆ›å»ºå›¾ä¾‹
        import matplotlib.lines as mlines
        
        handles = []
        for idx, model_name in enumerate(desired_order):
            line = mlines.Line2D([], [], 
                               color=colors[idx],
                               linestyle=line_styles[idx % len(line_styles)],
                               marker=marker_styles[idx % len(marker_styles)],
                               markersize=8,
                               linewidth=2.0,
                               label=model_name)
            handles.append(line)
        
        ax.legend(handles=handles,
                 loc='upper left',
                 fontsize=22,
                 frameon=True,
                 framealpha=0.9,
                 edgecolor='lightgray',
                 ncol=1)
        
        # ä¼˜åŒ–å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜ä¸ºPDF
        plt.savefig(output_file, format='pdf', bbox_inches='tight')
        plt.close()
        
        print(f"Visualization saved to {output_file}")


    def _compute_kl_divergence(self, P, Q):
        """
        Compute KL divergence between two distributions P and Q
        Using a small epsilon to avoid division by zero
        """
        epsilon = 1e-10
        P = P + epsilon
        Q = Q + epsilon
        
        # Normalize to make sure they sum to 1
        P = P / np.sum(P)
        Q = Q / np.sum(Q)
        
        return np.sum(P * np.log(P / Q))
    
    def _estimate_density(self, points, bins=20):
        """
        Estimate the probability density of points in 3D space
        using histogram binning
        """
        H, edges = np.histogramdd(points, bins=bins)
        return H / np.sum(H)
    
    def generate_report(self, analysis_results, output_file="analysis_report.txt"):
        """ç®€åŒ–çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•"""
        if not analysis_results:
            print("No valid analysis results!")
            return
            
        with open(output_file, "w") as f:
            f.write("=== Dialogue Analysis Report ===\n\n")
            
            # è®¡ç®—å„æ¨¡å‹å¹³å‡å·®å¼‚
            f.write("Model Performance Summary:\n")
            f.write("--------------------------\n")
            for model_name, embeddings in analysis_results['generated_embeddings'].items():
                target = analysis_results['target_embeddings'][:len(embeddings)]
                differences = np.linalg.norm(embeddings - target, axis=1)
                
                f.write(f"\n{model_name}:\n")
                f.write(f"  - Average Difference: {np.mean(differences):.4f}\n")
                f.write(f"  - Max Difference: {np.max(differences):.4f}\n")
                f.write(f"  - Min Difference: {np.min(differences):.4f}\n")
                f.write(f"  - Valid Dialogues: {len(differences)}\n")
            
            # æœ€ä½³è¡¨ç°æ¨¡å‹
            model_diffs = {
                model: np.mean(np.linalg.norm(embeddings - analysis_results['target_embeddings'][:len(embeddings)], axis=1))
                for model, embeddings in analysis_results['generated_embeddings'].items()
            }
            best_model = min(model_diffs, key=model_diffs.get)
            f.write(f"\nBest Performing Model: {best_model} (Avg Diff: {model_diffs[best_model]:.4f})\n")
            
        print(f"Report generated: {output_file}")
    
if __name__ == "__main__":
    start = time.time()
    
    print("\nâ³ Initializing analyzer...")
    analyzer = DialogueAnalyzer(model_dir="./model_cache")
    
    print("\nğŸš€ Analyzing dialogue data...")
    analysis_results = analyzer.analyze_dialogues("./merged_data.jsonl")
    
    print("\nğŸ“Š Generating visualizations...")
    analyzer.visualize_2d_differences(analysis_results)
    
    print("\nğŸ“ Generating analysis report...")
    analyzer.generate_report(analysis_results)
    
    print(f"\nâ±ï¸ Total time: {(time.time()-start)/60:.1f} minutes")
    print("\nâœ… Analysis complete! Output files:")
    print("- 2d_diff_visualization.png")
    print("- analysis_report.txt")
